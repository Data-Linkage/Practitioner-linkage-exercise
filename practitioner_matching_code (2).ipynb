{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Linkage Training Course\n",
    "\n",
    "We have two datasets which we want to link **practitioner_large_file** & **practitioner_small_file**:\n",
    "\n",
    "- **practitioner_large_file** contains the variables `IDA`, `Sex`, `Locality`, `Yearbirth`, `Monthbirth`, `Daybirth`, `Name` and a record ID that is contained in the variable `Bident`. In addition, there is a variable `Aident` that contains the record ID from the small file that it is matched to, i.e. we know the true match status.\n",
    "\n",
    "- **practitioner_small_file** contains the variables `IDB`, `Sex`, `Locality`, `Yearbirth`, `Monthbirth`, `Daybirth`, `Name` and a record ID that is contained in the variable `Aident`. In addition there is a variable, `Bident`, that contains the record ID from the large file that it is matched to, i.e. we know the true match status.\n",
    "\n",
    "Some of the variables in the **practitioner_small_file** were perturbed to simulate measurement errors. These are called `Sexpert`, `Yearbirthpert`, `Monthbirthpert`, `Daybirthpert`, `Namepert`. `Locality` was not perturbed and we will use this as a blocking variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pandas for data manipulation os to read the working directory\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Modify the settings so any variable or statement on its own line is displayed\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Widen output display\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def get_file_path():\n",
    "    # This is the filepath where the datasets and the matchkey file can be found\n",
    "    file_path = os.getcwd()\n",
    "    return file_path\n",
    "\n",
    "# Read in datasets to link. Get the filepath from the command line arguments\n",
    "filepath = get_file_path()\n",
    "\n",
    "dfA = pd.read_csv(filepath + '/practitioner_large_file.csv') # 1,000 rows\n",
    "dfB = pd.read_csv(filepath + '/practitioner_small_file.csv') # 680 rows\n",
    "\n",
    "# Make sure column types correct\n",
    "dfA = dfA.astype({\"SEX_A\": float,\n",
    "                  \"locality_A\": float,\n",
    "                  \"yearbirth_A\": float,\n",
    "                  \"monthbirth_A\": float,\n",
    "                  \"daybirth_A\": float,\n",
    "                  \"Name_A\": str})\n",
    "    \n",
    "dfB = dfB.astype({\"SEX_B\": float,\n",
    "                  \"locality_B\": float,\n",
    "                  \"yearbirth_B\": float,\n",
    "                  \"monthbirth_B\": float,\n",
    "                  \"daybirth_B\": float,\n",
    "                  \"Name_B\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX_A</th>\n",
       "      <th>locality_A</th>\n",
       "      <th>yearbirth_A</th>\n",
       "      <th>monthbirth_A</th>\n",
       "      <th>daybirth_A</th>\n",
       "      <th>Name_A</th>\n",
       "      <th>bident_A</th>\n",
       "      <th>aident_A</th>\n",
       "      <th>ID_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>geoff</td>\n",
       "      <td>E373</td>\n",
       "      <td>A1</td>\n",
       "      <td>2051588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>james</td>\n",
       "      <td>E371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2047429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Michael</td>\n",
       "      <td>E348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1926422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terry</td>\n",
       "      <td>E346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1907224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>MATTHEW</td>\n",
       "      <td>E332</td>\n",
       "      <td>A2</td>\n",
       "      <td>1818785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>micaell</td>\n",
       "      <td>E342</td>\n",
       "      <td>A3</td>\n",
       "      <td>1894191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neil</td>\n",
       "      <td>E341</td>\n",
       "      <td>A4</td>\n",
       "      <td>1887808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>matthew</td>\n",
       "      <td>E333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1825535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Kornell</td>\n",
       "      <td>E331</td>\n",
       "      <td>A5</td>\n",
       "      <td>1816773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>richard</td>\n",
       "      <td>E343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1898234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>JOE</td>\n",
       "      <td>E339</td>\n",
       "      <td>A6</td>\n",
       "      <td>1878042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>john</td>\n",
       "      <td>E338</td>\n",
       "      <td>A7</td>\n",
       "      <td>1867627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>eric</td>\n",
       "      <td>E345</td>\n",
       "      <td>A8</td>\n",
       "      <td>1905463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ross</td>\n",
       "      <td>E340</td>\n",
       "      <td>A9</td>\n",
       "      <td>1882068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>norman</td>\n",
       "      <td>E707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4082453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>daniel</td>\n",
       "      <td>E422</td>\n",
       "      <td>A10</td>\n",
       "      <td>2359327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>mr staley</td>\n",
       "      <td>E700</td>\n",
       "      <td>A11</td>\n",
       "      <td>4047561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Darren</td>\n",
       "      <td>E701</td>\n",
       "      <td>A12</td>\n",
       "      <td>4049253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>michael R</td>\n",
       "      <td>E349</td>\n",
       "      <td>A13</td>\n",
       "      <td>1938270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>geoffrey</td>\n",
       "      <td>E440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2486264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SEX_A  locality_A  yearbirth_A  monthbirth_A  daybirth_A     Name_A bident_A aident_A     ID_A\n",
       "0     1.0        31.0       1958.0          12.0         3.0      geoff     E373       A1  2051588\n",
       "1     1.0        31.0       1965.0           1.0        30.0      james     E371      NaN  2047429\n",
       "2     1.0        70.0       1922.0           7.0        28.0    Michael     E348      NaN  1926422\n",
       "3     1.0        70.0       1946.0           1.0         NaN      terry     E346      NaN  1907224\n",
       "4     1.0        70.0       1949.0           9.0         6.0    MATTHEW     E332       A2  1818785\n",
       "5     1.0        70.0       1961.0           9.0        19.0    micaell     E342       A3  1894191\n",
       "6     1.0        70.0       1961.0          12.0         3.0       Neil     E341       A4  1887808\n",
       "7     1.0        70.0       1963.0          11.0        26.0    matthew     E333      NaN  1825535\n",
       "8     1.0        70.0       1964.0          11.0        28.0    Kornell     E331       A5  1816773\n",
       "9     1.0        70.0       1967.0           8.0        27.0    richard     E343      NaN  1898234\n",
       "10    1.0        70.0       1970.0           7.0        23.0        JOE     E339       A6  1878042\n",
       "11    1.0        70.0       1972.0           7.0         1.0       john     E338       A7  1867627\n",
       "12    1.0        70.0       1976.0           7.0        16.0       eric     E345       A8  1905463\n",
       "13    1.0        70.0       1978.0          11.0        10.0       ross     E340       A9  1882068\n",
       "14    1.0        84.0       1957.0           9.0        13.0     norman     E707      NaN  4082453\n",
       "15    1.0        99.0       1976.0           6.0        14.0     daniel     E422      A10  2359327\n",
       "16    1.0       133.0       1920.0           5.0        28.0  mr staley     E700      A11  4047561\n",
       "17    1.0       134.0       1976.0           2.0        24.0     Darren     E701      A12  4049253\n",
       "18    1.0       166.0       1956.0           1.0         6.0  michael R     E349      A13  1938270\n",
       "19    1.0       171.0       1975.0          12.0         2.0   geoffrey     E440      NaN  2486264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX_B</th>\n",
       "      <th>locality_B</th>\n",
       "      <th>yearbirth_B</th>\n",
       "      <th>monthbirth_B</th>\n",
       "      <th>daybirth_B</th>\n",
       "      <th>Name_B</th>\n",
       "      <th>namepert_B</th>\n",
       "      <th>yearbirthpert_B</th>\n",
       "      <th>monthbirthpert_B</th>\n",
       "      <th>daybirthpert_B</th>\n",
       "      <th>sexpert_B</th>\n",
       "      <th>aident_B</th>\n",
       "      <th>bident_B</th>\n",
       "      <th>ID_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7400.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>alan</td>\n",
       "      <td>alan</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A245</td>\n",
       "      <td>E925</td>\n",
       "      <td>5196460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>JulianDickin</td>\n",
       "      <td>Julian</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>9</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A266</td>\n",
       "      <td>E20</td>\n",
       "      <td>97397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A377</td>\n",
       "      <td>E621</td>\n",
       "      <td>3563159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debbie</td>\n",
       "      <td>debbie</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A544</td>\n",
       "      <td>E903</td>\n",
       "      <td>5115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Mike</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A95</td>\n",
       "      <td>E291</td>\n",
       "      <td>1576027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>peter</td>\n",
       "      <td>deter</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A254</td>\n",
       "      <td>E885</td>\n",
       "      <td>5029982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>christine</td>\n",
       "      <td>christine</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A527</td>\n",
       "      <td>E191</td>\n",
       "      <td>945609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8700.0</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paul</td>\n",
       "      <td>paul</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A301</td>\n",
       "      <td>E464</td>\n",
       "      <td>2619977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>anita</td>\n",
       "      <td>anita</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A622</td>\n",
       "      <td>E255</td>\n",
       "      <td>1343394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>john</td>\n",
       "      <td>john</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A14</td>\n",
       "      <td>E627</td>\n",
       "      <td>3575957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dave</td>\n",
       "      <td>dave</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A21</td>\n",
       "      <td>E245</td>\n",
       "      <td>1300920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>emma</td>\n",
       "      <td>emma</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A464</td>\n",
       "      <td>E844</td>\n",
       "      <td>4756558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jacqueline</td>\n",
       "      <td>Jacqueline</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A655</td>\n",
       "      <td>E399</td>\n",
       "      <td>2211723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Linda</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A582</td>\n",
       "      <td>E323</td>\n",
       "      <td>1771384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>louisa</td>\n",
       "      <td>louisa</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A492</td>\n",
       "      <td>E130</td>\n",
       "      <td>628624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>tracy</td>\n",
       "      <td>tracy</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A654</td>\n",
       "      <td>E397</td>\n",
       "      <td>2188269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Linda</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A656</td>\n",
       "      <td>E398</td>\n",
       "      <td>2210393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Sherley</td>\n",
       "      <td>Sherley</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A639</td>\n",
       "      <td>E214</td>\n",
       "      <td>1087968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>philip</td>\n",
       "      <td>philip</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A152</td>\n",
       "      <td>E131</td>\n",
       "      <td>630146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>michael</td>\n",
       "      <td>micheal</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A32</td>\n",
       "      <td>E761</td>\n",
       "      <td>4352030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SEX_B  locality_B  yearbirth_B  monthbirth_B  daybirth_B        Name_B  namepert_B  yearbirthpert_B  monthbirthpert_B  daybirthpert_B  sexpert_B aident_B bident_B     ID_B\n",
       "0     1.0      7400.0       1975.0           7.0         6.0          alan        alan             75.0                 7             6.0        1.0     A245     E925  5196460\n",
       "1     1.0      7900.0       1975.0           9.0        30.0  JulianDickin      Julian           1076.0                 9            30.0        1.0     A266      E20    97397\n",
       "2     2.0       962.0       1900.0           6.0         NaN         Naomi       Naomi           1900.0                 6             NaN        2.0     A377     E621  3563159\n",
       "3     2.0      6500.0       1905.0           3.0         NaN        debbie      debbie           1905.0                 4             NaN        2.0     A544     E903  5115700\n",
       "4     1.0      3000.0       1905.0           3.0         NaN          Mike        Mike           1905.0                 3             NaN        1.0      A95     E291  1576027\n",
       "5     1.0      7800.0       1907.0           3.0        19.0         peter       deter           1907.0                 3            19.0        1.0     A254     E885  5029982\n",
       "6     2.0      6200.0       1908.0          12.0        20.0     christine   christine           1908.0                12            20.0        2.0     A527     E191   945609\n",
       "7     1.0      8700.0       1908.0           5.0         NaN          paul        paul           1908.0                 5             NaN        1.0     A301     E464  2619977\n",
       "8     2.0      8300.0       1909.0          11.0         6.0         anita       anita           1909.0                11             6.0        2.0     A622     E255  1343394\n",
       "9     1.0       188.0       1909.0           5.0         1.0          john        john           1909.0                 5             1.0        1.0      A14     E627  3575957\n",
       "10    1.0       466.0       1910.0           6.0         NaN          dave        dave           1910.0                 6             NaN        NaN      A21     E245  1300920\n",
       "11    2.0      4000.0       1910.0           3.0        28.0          emma        emma           1910.0                 3            28.0        2.0     A464     E844  4756558\n",
       "12    2.0      9000.0       1910.0           8.0         NaN    Jacqueline  Jacqueline           1910.0                 8             NaN        2.0     A655     E399  2211723\n",
       "13    2.0      7100.0       1910.0           7.0         NaN         Linda       Linda           1910.0                 7             NaN        2.0     A582     E323  1771384\n",
       "14    2.0      5000.0       1910.0           4.0         NaN        louisa      louisa           1910.0                 4             NaN        2.0     A492     E130   628624\n",
       "15    2.0      9000.0       1910.0           6.0        10.0         tracy       tracy           1910.0                 6            10.0        2.0     A654     E397  2188269\n",
       "16    2.0      9000.0       1911.0          12.0         9.0         Linda       Linda           1911.0                12             9.0        2.0     A656     E398  2210393\n",
       "17    2.0      8400.0       1911.0           2.0         7.0       Sherley     Sherley           1911.0                 2             7.0        2.0     A639     E214  1087968\n",
       "18    1.0      5000.0       1912.0          11.0        16.0        philip      philip           1912.0                11            16.0        1.0     A152     E131   630146\n",
       "19    1.0       516.0       1913.0          11.0         NaN       michael     micheal           1913.0                11             NaN        1.0      A32     E761  4352030"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns headings and first 20 rows\n",
    "dfA.head(20)\n",
    "dfB.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Exact Match using the unperturbed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert name variables in both datasets to uppercase before exact match\n",
    "dfA['Name_A'] = dfA['Name_A'].str.upper()\n",
    "dfB['Name_B'] = dfB['Name_B'].str.upper()\n",
    "\n",
    "# Exact match (inner join - links records from dfA and dfB if they agree on all the \n",
    "# variables we are using for matching, in this case sex, locality, year, month and day of birth and name)\n",
    "# Think of the inner part of a venn diagram.\n",
    "linked_df = dfA.merge(dfB,\n",
    "                      left_on = ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name_A'],\n",
    "                      right_on = ['SEX_B', 'locality_B', 'yearbirth_B', 'monthbirth_B', 'daybirth_B', 'Name_B'],\n",
    "                      how = 'inner')\n",
    "\n",
    "# Check the length of the linked dataset\n",
    "print('Number of records matched: ',len(linked_df))\n",
    "\n",
    "# Examine the residuals (outer join - gives us the records from dfA and dfB that do not\n",
    "# agree on all of the matching variables). Think of the outer circles in a venn diagram.\n",
    "residuals = dfA.merge(dfB,\n",
    "                      left_on = ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name_A'],\n",
    "                      right_on = ['SEX_B', 'locality_B', 'yearbirth_B', 'monthbirth_B', 'daybirth_B', 'Name_B'],\n",
    "                      how = 'outer',\n",
    "                      indicator = True)\n",
    "\n",
    "# Check residuals (i.e. records that do not have a match) from dfA\n",
    "residualsA = residuals[residuals['_merge'] == 'left_only'] \n",
    "print('Number of unmatched records in dataframeA: ',len(residualsA))\n",
    "\n",
    "# Check residuals from dfB\n",
    "residualsB = residuals[residuals['_merge'] == 'right_only'] \n",
    "print('Number of unmatched records in dataframeB: ',len(residualsB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all 680 records from the smaller dataset (`dfB`) linked to the larger dataset (`dfA`), as we have used the unperturbed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exact Match using the perturbed variables of `dfB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert peturbed name variable for dfB to uppercase before exact match\n",
    "dfB['namepert_B'] = dfB['namepert_B'].str.upper()\n",
    "\n",
    "# Exact match (inner join) using same variables (perturbed)\n",
    "linked_df2 = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'namepert_B'],\n",
    "                       how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Examine the residuals\n",
    "residuals2 = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'namepert_B'],\n",
    "                       how = 'outer',\n",
    "                      indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there were no unmatched records in `dfB` before. How many are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residuals from dfB\n",
    "residualsB = residuals2[residuals2['_merge'] == 'right_only'] \n",
    "print('Number of unmatched records in dataframeB: ',len(residualsB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, 174 records from the smaller dataset (`dfB`) did not link to a record in the larger dataset (`dfA`) due to errors in the data. Let's have a look at the residuals from `dfB` to see what types of errors caused names not to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View names of clean & perturbed dataset B residuals - errors may have prevented exact match\n",
    "residualsB[['Name_B', 'namepert_B']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of errors in the data can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Clean the data by parsing names into two where appropriate and stripping off titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the name variables in both datasets. For example 'John Smith' would be split into two variables `name1` and `name2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new columns name1 and name2, by splitting the name column by the delimiter ' '\n",
    "# Name2 will be 'None' if there was only one name\n",
    "# For dfB use namepert instead of name\n",
    "dfA[['Name1_A', 'Name2_A']] = dfA['Name_A'].str.split(' ',expand=True)\n",
    "dfB[['Name1pert_B', 'Name2pert_B']] = dfB['namepert_B'].str.split(' ',expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete redundant words. In this case, we only have first names in the file. There are prefixes that appear in the dataset, such as: Mr. Smith or Mrs. Smith. Since we only have one field for name in the dataset, we will delete the redundant word and put the more ‘relevant’ name in  that  field. In this  simple case, the only redundant words in our datasets  are ‘MR’ and  ‘MRS’, so we will simply use a `where` / `if-else` statement in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap Name 1 and Name 2 if Name 1 is Mr or Mrs\n",
    "import numpy as np\n",
    "dfA['Name1_A'], dfA['Name2_A'] = np.where((dfA['Name1_A'] == 'MR') | (dfA['Name1_A'] == 'MRS'), [dfA['Name2_A'], dfA['Name1_A']], [dfA['Name1_A'], dfA['Name2_A']])\n",
    "dfB['Name1pert_B'], dfB['Name2pert_B'] = np.where((dfB['Name1pert_B'] == 'MR') | (dfB['Name1pert_B'] == 'MRS'), [dfB['Name2pert_B'], dfB['Name1pert_B']], [dfB['Name1pert_B'], dfB['Name2pert_B']])\n",
    "\n",
    "# Set Name 2 to missing if it is Mr or Mrs\n",
    "dfA['Name2_A'] = np.where((dfA['Name2_A'] == 'MR') | (dfA['Name2_A'] == 'MRS'), None, dfA['Name2_A'])\n",
    "dfB['Name2pert_B'] = np.where((dfB['Name2pert_B'] == 'MR') | (dfB['Name2pert_B'] == 'MRS'), None, dfB['Name2pert_B'])\n",
    "\n",
    "# Have a look at dataframe A to check the changes we've made\n",
    "dfA[['Name1_A', 'Name2_A']].head(20)\n",
    "\n",
    "\n",
    "# Exact match (inner join) using the variables (perturbed) after cleaning\n",
    "linked_df3 = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name1_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'Name1pert_B'],\n",
    "                       how = 'inner')\n",
    "\n",
    "# Find the residuals\n",
    "residuals3 = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Name1_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'Name1pert_B'],\n",
    "                       how = 'outer',\n",
    "                       indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there were 174 unmatched records in `dfB` before. How many are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residuals from dfB\n",
    "residualsB = residuals3[residuals3['_merge'] == 'right_only']\n",
    "print('Number of unmatched records in dataframeB: ',len(residualsB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved things a little by cleaning the `name` strings. But there are still 166 unmatched records in `dfB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the residuals - what kind of errors in the data do we still have?\n",
    "residualsB[['Name_B', 'Name1pert_B']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4a: Rule-Based Matching - Relax Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have used exact matching. If we relax the rules a little we should find more matches. But beware - we may also introduce some false positives. What happens if we relax the criteria for `name` to match to only insisting that the first three letters of `name` match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new columns containing only the first three letters of the name in each dataset\n",
    "dfA['Short_Name_A'] = dfA.Name1_A.str[0:3]\n",
    "dfB['Short_Name_B'] = dfB.Name1pert_B.str[0:3]\n",
    "\n",
    "# Join on the new Short Name variable and sex, locality and date of birth\n",
    "linked_df4a = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Short_Name_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'Short_Name_B'],\n",
    "                       how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "# Find the residuals\n",
    "residuals4a = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'yearbirth_A', 'monthbirth_A', 'daybirth_A', 'Short_Name_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B', 'yearbirthpert_B', 'monthbirthpert_B', 'daybirthpert_B', 'Short_Name_B'],\n",
    "                       how = 'outer',\n",
    "                       indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there were 166 unmatched records in `dfB` before. How many are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count residuals from dfB\n",
    "residualsB = residuals4a[residuals4a['_merge'] == 'right_only']\n",
    "print('Number of unmatched records in dataframeB: ',len(residualsB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we only have 114 unmatched records - BUT are all of the matches correct? Check to see if `IDA=IDB` for all\n",
    "the records in the linked dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'Match_Status' that has value 1 if the match is correct and 0 otherwise\n",
    "linked_df4a['Match_Status'] = np.where(linked_df4a['ID_A'] == linked_df4a['ID_B'],1,0)\n",
    "\n",
    "print('Number of correct matches: ', len(linked_df4a[linked_df4a['Match_Status']==1]))\n",
    "print('Number of false positives: ', len(linked_df4a[linked_df4a['Match_Status']==0]))\n",
    "\n",
    "# Examine the false positives\n",
    "FPs = linked_df4a[linked_df4a['Match_Status']==0]\n",
    "\n",
    "# View\n",
    "FPs[['SEX_A', 'sexpert_B',\n",
    "     'yearbirth_A', 'yearbirthpert_B',\n",
    "     'monthbirth_A', 'monthbirthpert_B',\n",
    "     'daybirth_A', 'daybirthpert_B',\n",
    "     'Name_A', 'namepert_B']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five false positives. Go back to the start of step 4a and change the length of the `Short_Name_A` and `Short_Name_B`. What is the best length of string to use to find the most true positives with the least false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4b: Rule-Based Matching - Relax Date of Birth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we relax the criteria for date of birth. Up until now we have insisted that day, month and year all match exactly. Let's see what happens if we let day be a mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_df4b = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'monthbirth_A','yearbirth_A',  'Name1_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B',  'monthbirthpert_B', 'yearbirthpert_B','Name1pert_B'],\n",
    "                       how = 'inner')\n",
    "\n",
    "# Find the residuals\n",
    "residuals4b = dfA.merge(dfB,\n",
    "                       left_on =  ['SEX_A', 'locality_A', 'monthbirth_A','yearbirth_A',  'Name1_A'],\n",
    "                       right_on = ['sexpert_B', 'locality_B',  'monthbirthpert_B', 'yearbirthpert_B','Name1pert_B'],\n",
    "                       how = 'outer',\n",
    "                       indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many residuals are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residuals from dfB\n",
    "residualsB = residuals4b[residuals4b['_merge'] == 'right_only']\n",
    "print('Number of unmatched records in dataframeB: ',len(residualsB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the matches are correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if ID_A=ID_B for all the records in the linked dataset\n",
    "#Add a column 'Match_Status' that has value 1 if the match is correct and 0 otherwise\n",
    "linked_df4b['Match_Status'] = np.where(linked_df4b['ID_A'] == linked_df4b['ID_B'],1,0)\n",
    "\n",
    "print('Number of correct matches: ', len(linked_df4b[linked_df4b['Match_Status']==1]))\n",
    "print('Number of false positives: ', len(linked_df4b[linked_df4b['Match_Status']==0]))\n",
    "\n",
    "#Examine the false positives\n",
    "FPs = linked_df4b[linked_df4b['Match_Status']==0]\n",
    "\n",
    "# View\n",
    "FPs[['SEX_A', 'sexpert_B',\n",
    "     'yearbirth_A', 'yearbirthpert_B',\n",
    "     'monthbirth_A', 'monthbirthpert_B',\n",
    "     'daybirth_A', 'daybirthpert_B',\n",
    "     'Name_A', 'namepert_B']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a false positive. Change the variables in the merges to see if we should remove a different, or another, date of birth variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - using Levenshtein edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function LD returns the Levenshtein edit distance between two strings\n",
    "def LD(s, t):\n",
    "    if s == \"\":\n",
    "        return len(t);\n",
    "    if t == \"\":\n",
    "        return len(s);\n",
    "    if s[-1] == t[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "       \n",
    "    res = min([LD(s[:-1], t)+1,\n",
    "               LD(s, t[:-1])+1, \n",
    "               LD(s[:-1], t[:-1]) + cost])\n",
    "    return res;\n",
    "\n",
    "\n",
    "#Test the function by inputting some words of your choice\n",
    "string1=\"Rachel\"\n",
    "string2=\"Rachael\"\n",
    "print('Levenshtein Edit Distance [',string1,',',string2,'] = ',LD(string1, string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `[Rachel, Rachael]` and `[Tim, Tom]` both have a `LD` score of 1. But one change in a three letter word is more significant than one change in a six letter word. To account for different name lengths, we'll also calculate a standardised Levenshtein Edit Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLD(s,t):\n",
    "    standardised = 1-(LD(s,t)/max(len(s),len(t)))\n",
    "    return standardised;\n",
    "\n",
    "string1=\"Rachel\"\n",
    "string2=\"Rachael\"\n",
    "print('Standardised Levenshtein Edit Distance [',string1,',',string2,'] = ',SLD(string1, string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Levenshtein distance as part of score based matching in Step 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assume we don’t have a blocking variable. We will generate all possible pairs in the two datasets using the perturbed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do this by creating two columns containing a single value and then join on that value\n",
    "dfA['join_col'] = 0\n",
    "dfB['join_col'] = 0\n",
    "\n",
    "# Produce all possible candidate pairs\n",
    "CP1 = dfA.merge(dfB,\n",
    "                on = 'join_col',\n",
    "                how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dfA` contains 1,000 records and `dfB` contains 680 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(CP1), 'possible pairs. Of these, 680 (0.1%) are true matches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the blocking variable `locality`. This isn't perturbed hence we expect to retain all 680 true matches (and also find extra, false matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce candidate pairs using blocking variable locality\n",
    "CP2 = dfA.merge(dfB,\n",
    "                left_on = 'locality_A',\n",
    "                right_on = 'locality_B',\n",
    "                how = 'left')\n",
    "\n",
    "print('There are', len(CP2), 'possible pairs. Of these, 680 (3.6%) are true matches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Score Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign weights to the variables used for matching\n",
    "name_w=5\n",
    "sex_w=1\n",
    "day_w=2\n",
    "month_w=2\n",
    "year_w=3\n",
    "\n",
    "CP1['Score1']=(((CP1.Name1_A==CP1.Name1pert_B)*name_w)\n",
    "              +((CP1.SEX_A==CP1.sexpert_B)*sex_w)\n",
    "              +(((CP1.daybirth_A==CP1.daybirthpert_B) |(np.isnan(CP1.daybirth_A) & np.isnan(CP1.daybirthpert_B)))*day_w)\n",
    "              +(((CP1.monthbirth_A==CP1.monthbirthpert_B) | (np.isnan(CP1.monthbirth_A) & np.isnan(CP1.monthbirthpert_B)))*month_w)\n",
    "              +(((CP1.yearbirth_A==CP1.yearbirthpert_B) | (np.isnan(CP1.yearbirth_A) & np.isnan(CP1.yearbirthpert_B)))*year_w))             \n",
    "\n",
    "#This will tell us how many candidate pairs scored each of 0 (complete mismatch) to 13 (exact match)\n",
    "CP1.Score1.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 514 exact matches as expected - same as the number of residials after `name` cleaning and exact matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP2['Score2']=(((CP2.Name1_A==CP2.Name1pert_B)*name_w)\n",
    "              +((CP2.SEX_A==CP2.sexpert_B)*sex_w)\n",
    "              +(((CP2.daybirth_A==CP2.daybirthpert_B) |(np.isnan(CP2.daybirth_A) & np.isnan(CP2.daybirthpert_B)))*day_w)\n",
    "              +(((CP2.monthbirth_A==CP2.monthbirthpert_B) | (np.isnan(CP2.monthbirth_A) & np.isnan(CP2.monthbirthpert_B)))*month_w)\n",
    "              +(((CP2.yearbirth_A==CP2.yearbirthpert_B) | (np.isnan(CP2.yearbirth_A) & np.isnan(CP2.yearbirthpert_B)))*year_w))                \n",
    "           \n",
    "\n",
    "#This will tell us how many candidate pairs scored each of 0 (complete mismatch) to 13 (exact match)\n",
    "CP2.Score2.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the threshold for accepting as a match\n",
    "Threshold=10\n",
    "Matches = CP2[CP2['Score2'] >= Threshold]\n",
    "print('With threshold', Threshold, 'there are ',len(Matches), 'matches.')\n",
    "Matches.loc[:,'Match_Status']=np.where(Matches['ID_A'] == Matches['ID_B'],1,0)   # ignore warning\n",
    "\n",
    "# TP and FP\n",
    "print('Number of correct matches: ', len(Matches[Matches['Match_Status']==1]))\n",
    "print('Number of false positives: ', len(Matches[Matches['Match_Status']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the different scores tell us? You can alter the weights and threshold until you are happy that you have maximised true positives and minimised false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Score Based Matching with partial scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the Levenshtein Edit Distance to award a partial score for names that partially match. So names like Rachel and Rachael will score `0.857*name_w` rather than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with blank space\n",
    "CP2['Name1_A'] = np.where(CP2['Name1_A'].isnull(), '', CP2['Name1_A'])\n",
    "CP2['Name1pert_B'] = np.where(CP2['Name1pert_B'].isnull(), '', CP2['Name1pert_B'])\n",
    "\n",
    "# Apply edit distance to pairs (warning - this takes about 12 minutes)\n",
    "CP2['SLD']=CP2.apply(lambda x: SLD(x['Name1_A'], x['Name1pert_B']), axis=1)\n",
    "\n",
    "# Calculate score\n",
    "CP2['Score2']=((CP2.SLD*name_w)\n",
    "              +((CP2.SEX_A==CP2.SEX_B)*sex_w)\n",
    "              +((CP2.daybirth_A==CP2.daybirthpert_B)*day_w)\n",
    "              +((CP2.monthbirth_A==CP2.monthbirthpert_B)*month_w)\n",
    "              +((CP2.yearbirth_A==CP2.yearbirthpert_B)*year_w))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold=10\n",
    "Matches2 = CP2[CP2['Score2']>=Threshold]\n",
    "print('With threshold', Threshold ,'there are',len(Matches2), 'matches.')\n",
    "Matches2.loc[:,'Match_Status']=np.where(Matches2['ID_A'] == Matches2['ID_B'],1,0)\n",
    "\n",
    "# TP and FP\n",
    "print('Number of correct matches: ', len(Matches2[Matches2['Match_Status']==1]))\n",
    "print('Number of false positives: ', len(Matches2[Matches2['Match_Status']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `Threshold` to see if you can make the false postive rate smaller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
